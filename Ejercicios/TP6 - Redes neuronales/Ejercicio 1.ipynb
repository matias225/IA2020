{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=109)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "Clasificar los datos correspondientes al dataset de pacientes con cancer utilizando un perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de iteraciones: 27\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = Perceptron(\n",
    "             # random_state=42,\n",
    "              max_iter=150,\n",
    "              tol=0.001              \n",
    "              )\n",
    "p.fit(X_train, y_train)\n",
    "print(\"Cantidad de iteraciones: \" +str(p.n_iter_))\n",
    "disp = metrics.plot_confusion_matrix(p, X_test, y_test,cmap=plt.cm.Blues)\n",
    "disp.ax_.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿Es posible clasificar correctamente los casos utilizando un Perceptron?\n",
    "Si es posible clasificar utilizando un Perceptron\n",
    "- ¿Cuántas iteraciones son necesarias?\n",
    "27 iteraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si es posible clasificar correctamente utilizando un Perceptron\n",
    "Son necesarias 27 iteraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron multicapa\n",
    "\n",
    "Clasificar los datos correspondientes al dataset de pacientes con cancer utilizando un perceptron multicapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = inf\n",
      "Iteration 3, loss = inf\n",
      "Iteration 4, loss = inf\n",
      "Iteration 5, loss = inf\n",
      "Iteration 6, loss = inf\n",
      "Iteration 7, loss = inf\n",
      "Iteration 8, loss = inf\n",
      "Iteration 9, loss = inf\n",
      "Iteration 10, loss = inf\n",
      "Iteration 11, loss = inf\n",
      "Iteration 12, loss = inf\n",
      "Iteration 13, loss = inf\n",
      "Iteration 14, loss = inf\n",
      "Iteration 15, loss = inf\n",
      "Iteration 16, loss = inf\n",
      "Iteration 17, loss = inf\n",
      "Iteration 18, loss = inf\n",
      "Iteration 19, loss = inf\n",
      "Iteration 20, loss = inf\n",
      "Iteration 21, loss = inf\n",
      "Iteration 22, loss = inf\n",
      "Iteration 23, loss = inf\n",
      "Iteration 24, loss = inf\n",
      "Iteration 25, loss = inf\n",
      "Iteration 26, loss = inf\n",
      "Iteration 27, loss = inf\n",
      "Iteration 28, loss = inf\n",
      "Iteration 29, loss = inf\n",
      "Iteration 30, loss = inf\n",
      "Iteration 31, loss = inf\n",
      "Iteration 32, loss = inf\n",
      "Iteration 33, loss = inf\n",
      "Iteration 34, loss = inf\n",
      "Iteration 35, loss = inf\n",
      "Iteration 36, loss = inf\n",
      "Iteration 37, loss = inf\n",
      "Iteration 38, loss = inf\n",
      "Iteration 39, loss = inf\n",
      "Iteration 40, loss = inf\n",
      "Iteration 41, loss = inf\n",
      "Iteration 42, loss = inf\n",
      "Iteration 43, loss = inf\n",
      "Iteration 44, loss = inf\n",
      "Iteration 45, loss = inf\n",
      "Iteration 46, loss = inf\n",
      "Iteration 47, loss = inf\n",
      "Iteration 48, loss = inf\n",
      "Iteration 49, loss = inf\n",
      "Iteration 50, loss = inf\n",
      "Iteration 51, loss = inf\n",
      "Iteration 52, loss = inf\n",
      "Iteration 53, loss = inf\n",
      "Iteration 54, loss = inf\n",
      "Iteration 55, loss = inf\n",
      "Iteration 56, loss = inf\n",
      "Iteration 57, loss = inf\n",
      "Iteration 58, loss = inf\n",
      "Iteration 59, loss = inf\n",
      "Iteration 60, loss = inf\n",
      "Iteration 61, loss = inf\n",
      "Iteration 62, loss = inf\n",
      "Iteration 63, loss = inf\n",
      "Iteration 64, loss = inf\n",
      "Iteration 65, loss = inf\n",
      "Iteration 66, loss = inf\n",
      "Iteration 67, loss = inf\n",
      "Iteration 68, loss = inf\n",
      "Iteration 69, loss = inf\n",
      "Iteration 70, loss = inf\n",
      "Iteration 71, loss = inf\n",
      "Iteration 72, loss = inf\n",
      "Iteration 73, loss = inf\n",
      "Iteration 74, loss = inf\n",
      "Iteration 75, loss = inf\n",
      "Iteration 76, loss = inf\n",
      "Iteration 77, loss = inf\n",
      "Iteration 78, loss = inf\n",
      "Iteration 79, loss = inf\n",
      "Iteration 80, loss = inf\n",
      "Iteration 81, loss = inf\n",
      "Iteration 82, loss = inf\n",
      "Iteration 83, loss = inf\n",
      "Iteration 84, loss = inf\n",
      "Iteration 85, loss = inf\n",
      "Iteration 86, loss = inf\n",
      "Iteration 87, loss = inf\n",
      "Iteration 88, loss = inf\n",
      "Iteration 89, loss = inf\n",
      "Iteration 90, loss = inf\n",
      "Iteration 91, loss = inf\n",
      "Iteration 92, loss = inf\n",
      "Iteration 93, loss = inf\n",
      "Iteration 94, loss = inf\n",
      "Iteration 95, loss = inf\n",
      "Iteration 96, loss = inf\n",
      "Iteration 97, loss = inf\n",
      "Iteration 98, loss = inf\n",
      "Iteration 99, loss = inf\n",
      "Iteration 100, loss = inf\n",
      "Iteration 101, loss = inf\n",
      "Iteration 102, loss = inf\n",
      "Iteration 103, loss = inf\n",
      "Iteration 104, loss = inf\n",
      "Iteration 105, loss = inf\n",
      "Iteration 106, loss = inf\n",
      "Iteration 107, loss = inf\n",
      "Iteration 108, loss = inf\n",
      "Iteration 109, loss = inf\n",
      "Iteration 110, loss = inf\n",
      "Iteration 111, loss = inf\n",
      "Iteration 112, loss = inf\n",
      "Iteration 113, loss = inf\n",
      "Iteration 114, loss = inf\n",
      "Iteration 115, loss = inf\n",
      "Iteration 116, loss = inf\n",
      "Iteration 117, loss = inf\n",
      "Iteration 118, loss = inf\n",
      "Iteration 119, loss = inf\n",
      "Iteration 120, loss = inf\n",
      "Iteration 121, loss = inf\n",
      "Iteration 122, loss = inf\n",
      "Iteration 123, loss = inf\n",
      "Iteration 124, loss = inf\n",
      "Iteration 125, loss = inf\n",
      "Iteration 126, loss = inf\n",
      "Iteration 127, loss = inf\n",
      "Iteration 128, loss = inf\n",
      "Iteration 129, loss = inf\n",
      "Iteration 130, loss = inf\n",
      "Iteration 131, loss = inf\n",
      "Iteration 132, loss = inf\n",
      "Iteration 133, loss = inf\n",
      "Iteration 134, loss = inf\n",
      "Iteration 135, loss = inf\n",
      "Iteration 136, loss = inf\n",
      "Iteration 137, loss = inf\n",
      "Iteration 138, loss = inf\n",
      "Iteration 139, loss = inf\n",
      "Iteration 140, loss = inf\n",
      "Iteration 141, loss = inf\n",
      "Iteration 142, loss = inf\n",
      "Iteration 143, loss = inf\n",
      "Iteration 144, loss = inf\n",
      "Iteration 145, loss = inf\n",
      "Iteration 146, loss = inf\n",
      "Iteration 147, loss = inf\n",
      "Iteration 148, loss = inf\n",
      "Iteration 149, loss = inf\n",
      "Iteration 150, loss = 1.21336328\n",
      "Iteration 151, loss = 1.20422988\n",
      "Iteration 152, loss = 1.19461854\n",
      "Iteration 153, loss = 1.18488205\n",
      "Iteration 154, loss = 1.17663766\n",
      "Iteration 155, loss = 1.16774829\n",
      "Iteration 156, loss = 1.15516627\n",
      "Iteration 157, loss = 1.14750939\n",
      "Iteration 158, loss = 1.13691387\n",
      "Iteration 159, loss = 1.12675646\n",
      "Iteration 160, loss = 1.12360833\n",
      "Iteration 161, loss = 1.11113625\n",
      "Iteration 162, loss = 1.10259309\n",
      "Iteration 163, loss = 1.09166354\n",
      "Iteration 164, loss = 1.08728580\n",
      "Iteration 165, loss = 1.07670501\n",
      "Iteration 166, loss = 1.06456356\n",
      "Iteration 167, loss = 1.05532367\n",
      "Iteration 168, loss = 1.04883809\n",
      "Iteration 169, loss = 1.03981537\n",
      "Iteration 170, loss = 1.03024959\n",
      "Iteration 171, loss = 1.02338027\n",
      "Iteration 172, loss = 1.01506363\n",
      "Iteration 173, loss = 1.00674806\n",
      "Iteration 174, loss = 1.00084669\n",
      "Iteration 175, loss = 0.98873427\n",
      "Iteration 176, loss = 0.98269737\n",
      "Iteration 177, loss = 0.97375520\n",
      "Iteration 178, loss = 0.96914962\n",
      "Iteration 179, loss = 0.96023227\n",
      "Iteration 180, loss = 0.95567085\n",
      "Iteration 181, loss = 0.95015450\n",
      "Iteration 182, loss = 0.94782201\n",
      "Iteration 183, loss = 0.94059536\n",
      "Iteration 184, loss = 0.93628131\n",
      "Iteration 185, loss = 0.93198245\n",
      "Iteration 186, loss = 0.92833949\n",
      "Iteration 187, loss = 0.92430948\n",
      "Iteration 188, loss = 0.92142147\n",
      "Iteration 189, loss = 0.91830141\n",
      "Iteration 190, loss = 0.91361751\n",
      "Iteration 191, loss = 0.91030517\n",
      "Iteration 192, loss = 0.90808434\n",
      "Iteration 193, loss = 0.90809219\n",
      "Iteration 194, loss = 0.90168096\n",
      "Iteration 195, loss = 0.89719150\n",
      "Iteration 196, loss = 0.89374489\n",
      "Iteration 197, loss = 0.89019394\n",
      "Iteration 198, loss = 0.88689109\n",
      "Iteration 199, loss = 0.88355555\n",
      "Iteration 200, loss = 0.88248307\n",
      "Iteration 201, loss = 0.87793561\n",
      "Iteration 202, loss = 0.87570239\n",
      "Iteration 203, loss = 0.87103902\n",
      "Iteration 204, loss = 0.87028213\n",
      "Iteration 205, loss = 0.86476209\n",
      "Iteration 206, loss = 0.86516692\n",
      "Iteration 207, loss = 0.85833179\n",
      "Iteration 208, loss = 0.85482713\n",
      "Iteration 209, loss = 0.85142867\n",
      "Iteration 210, loss = 0.84856767\n",
      "Iteration 211, loss = 0.84586973\n",
      "Iteration 212, loss = 0.84217858\n",
      "Iteration 213, loss = 0.83896166\n",
      "Iteration 214, loss = 0.83826059\n",
      "Iteration 215, loss = 0.83465247\n",
      "Iteration 216, loss = 0.82999490\n",
      "Iteration 217, loss = 0.82654471\n",
      "Iteration 218, loss = 0.82312168\n",
      "Iteration 219, loss = 0.82238245\n",
      "Iteration 220, loss = 0.81680712\n",
      "Iteration 221, loss = 0.81469536\n",
      "Iteration 222, loss = 0.81160950\n",
      "Iteration 223, loss = 0.80755029\n",
      "Iteration 224, loss = 0.80395721\n",
      "Iteration 225, loss = 0.80021692\n",
      "Iteration 226, loss = 0.79748231\n",
      "Iteration 227, loss = 0.79500748\n",
      "Iteration 228, loss = 0.79397138\n",
      "Iteration 229, loss = 0.78796721\n",
      "Iteration 230, loss = 0.78551796\n",
      "Iteration 231, loss = 0.78412457\n",
      "Iteration 232, loss = 0.77883403\n",
      "Iteration 233, loss = 0.77539214\n",
      "Iteration 234, loss = 0.77197208\n",
      "Iteration 235, loss = 0.77057712\n",
      "Iteration 236, loss = 0.76992936\n",
      "Iteration 237, loss = 0.76321727\n",
      "Iteration 238, loss = 0.76045981\n",
      "Iteration 239, loss = 0.75631779\n",
      "Iteration 240, loss = 0.75269825\n",
      "Iteration 241, loss = 0.75006293\n",
      "Iteration 242, loss = 0.74704206\n",
      "Iteration 243, loss = 0.74455147\n",
      "Iteration 244, loss = 0.74056729\n",
      "Iteration 245, loss = 0.73721735\n",
      "Iteration 246, loss = 0.73461958\n",
      "Iteration 247, loss = 0.73337661\n",
      "Iteration 248, loss = 0.72874996\n",
      "Iteration 249, loss = 0.72501300\n",
      "Iteration 250, loss = 0.72159884\n",
      "Iteration 251, loss = 0.71888155\n",
      "Iteration 252, loss = 0.71654819\n",
      "Iteration 253, loss = 0.71332205\n",
      "Iteration 254, loss = 0.71076989\n",
      "Iteration 255, loss = 0.70610953\n",
      "Iteration 256, loss = 0.70501978\n",
      "Iteration 257, loss = 0.70148817\n",
      "Iteration 258, loss = 0.69787716\n",
      "Iteration 259, loss = 0.69647232\n",
      "Iteration 260, loss = 0.69234003\n",
      "Iteration 261, loss = 0.68813764\n",
      "Iteration 262, loss = 0.68540205\n",
      "Iteration 263, loss = 0.68296188\n",
      "Iteration 264, loss = 0.67904933\n",
      "Iteration 265, loss = 0.67635769\n",
      "Iteration 266, loss = 0.67405709\n",
      "Iteration 267, loss = 0.67082511\n",
      "Iteration 268, loss = 0.67412240\n",
      "Iteration 269, loss = 0.66571287\n",
      "Iteration 270, loss = 0.66106398\n",
      "Iteration 271, loss = 0.65863634\n",
      "Iteration 272, loss = 0.65630174\n",
      "Iteration 273, loss = 0.65258156\n",
      "Iteration 274, loss = 0.64954597\n",
      "Iteration 275, loss = 0.64866564\n",
      "Iteration 276, loss = 0.64592323\n",
      "Iteration 277, loss = 0.64283512\n",
      "Iteration 278, loss = 0.64279111\n",
      "Iteration 279, loss = 0.63543069\n",
      "Iteration 280, loss = 0.63563881\n",
      "Iteration 281, loss = 0.63187853\n",
      "Iteration 282, loss = 0.62745023\n",
      "Iteration 283, loss = 0.62415292\n",
      "Iteration 284, loss = 0.62089416\n",
      "Iteration 285, loss = 0.61977577\n",
      "Iteration 286, loss = 0.61632119\n",
      "Iteration 287, loss = 0.61394750\n",
      "Iteration 288, loss = 0.61001454\n",
      "Iteration 289, loss = 0.60747732\n",
      "Iteration 290, loss = 0.60488272\n",
      "Iteration 291, loss = 0.60346078\n",
      "Iteration 292, loss = 0.59924628\n",
      "Iteration 293, loss = 0.59682231\n",
      "Iteration 294, loss = 0.59446115\n",
      "Iteration 295, loss = 0.59117915\n",
      "Iteration 296, loss = 0.58927360\n",
      "Iteration 297, loss = 0.58645970\n",
      "Iteration 298, loss = 0.58411548\n",
      "Iteration 299, loss = 0.58248589\n",
      "Iteration 300, loss = 0.58041051\n",
      "Iteration 301, loss = 0.57626933\n",
      "Iteration 302, loss = 0.57453338\n",
      "Iteration 303, loss = 0.57086299\n",
      "Iteration 304, loss = 0.56848727\n",
      "Iteration 305, loss = 0.56542910\n",
      "Iteration 306, loss = 0.56274451\n",
      "Iteration 307, loss = 0.56147777\n",
      "Iteration 308, loss = 0.55831913\n",
      "Iteration 309, loss = 0.55530615\n",
      "Iteration 310, loss = 0.55401847\n",
      "Iteration 311, loss = 0.55145573\n",
      "Iteration 312, loss = 0.54839213\n",
      "Iteration 313, loss = 0.54547645\n",
      "Iteration 314, loss = 0.54350795\n",
      "Iteration 315, loss = 0.54356552\n",
      "Iteration 316, loss = 0.53992326\n",
      "Iteration 317, loss = 0.53753129\n",
      "Iteration 318, loss = 0.53344609\n",
      "Iteration 319, loss = 0.53560470\n",
      "Iteration 320, loss = 0.53025764\n",
      "Iteration 321, loss = 0.52818371\n",
      "Iteration 322, loss = 0.52536974\n",
      "Iteration 323, loss = 0.52208398\n",
      "Iteration 324, loss = 0.52194536\n",
      "Iteration 325, loss = 0.51797509\n",
      "Iteration 326, loss = 0.51481338\n",
      "Iteration 327, loss = 0.51264533\n",
      "Iteration 328, loss = 0.51036425\n",
      "Iteration 329, loss = 0.50909664\n",
      "Iteration 330, loss = 0.50626128\n",
      "Iteration 331, loss = 0.50691605\n",
      "Iteration 332, loss = 0.50649345\n",
      "Iteration 333, loss = 0.49974509\n",
      "Iteration 334, loss = 0.49634799\n",
      "Iteration 335, loss = 0.49568964\n",
      "Iteration 336, loss = 0.49337823\n",
      "Iteration 337, loss = 0.48998627\n",
      "Iteration 338, loss = 0.48744368\n",
      "Iteration 339, loss = 0.48624024\n",
      "Iteration 340, loss = 0.48465989\n",
      "Iteration 341, loss = 0.48289629\n",
      "Iteration 342, loss = 0.48116167\n",
      "Iteration 343, loss = 0.47734779\n",
      "Iteration 344, loss = 0.47619981\n",
      "Iteration 345, loss = 0.47311854\n",
      "Iteration 346, loss = 0.47012229\n",
      "Iteration 347, loss = 0.47092548\n",
      "Iteration 348, loss = 0.47029616\n",
      "Iteration 349, loss = 0.46516678\n",
      "Iteration 350, loss = 0.46265698\n",
      "Iteration 351, loss = 0.45992910\n",
      "Iteration 352, loss = 0.45902535\n",
      "Iteration 353, loss = 0.45651331\n",
      "Iteration 354, loss = 0.45486890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 355, loss = 0.45302946\n",
      "Iteration 356, loss = 0.45092020\n",
      "Iteration 357, loss = 0.45027264\n",
      "Iteration 358, loss = 0.45039019\n",
      "Iteration 359, loss = 0.44512660\n",
      "Iteration 360, loss = 0.44221497\n",
      "Iteration 361, loss = 0.44921659\n",
      "Iteration 362, loss = 0.44486304\n",
      "Iteration 363, loss = 0.43692265\n",
      "Iteration 364, loss = 0.43760539\n",
      "Iteration 365, loss = 0.43371987\n",
      "Iteration 366, loss = 0.42833644\n",
      "Iteration 367, loss = 0.42853519\n",
      "Iteration 368, loss = 0.42835458\n",
      "Iteration 369, loss = 0.42482897\n",
      "Iteration 370, loss = 0.42600044\n",
      "Iteration 371, loss = 0.42163159\n",
      "Iteration 372, loss = 0.42096289\n",
      "Iteration 373, loss = 0.41508571\n",
      "Iteration 374, loss = 0.41230453\n",
      "Iteration 375, loss = 0.41325760\n",
      "Iteration 376, loss = 0.41258991\n",
      "Iteration 377, loss = 0.40869584\n",
      "Iteration 378, loss = 0.40571816\n",
      "Iteration 379, loss = 0.40439428\n",
      "Iteration 380, loss = 0.40294294\n",
      "Iteration 381, loss = 0.39946788\n",
      "Iteration 382, loss = 0.39787986\n",
      "Iteration 383, loss = 0.40728946\n",
      "Iteration 384, loss = 0.39402043\n",
      "Iteration 385, loss = 0.39214055\n",
      "Iteration 386, loss = 0.39412946\n",
      "Iteration 387, loss = 0.38934851\n",
      "Iteration 388, loss = 0.39185632\n",
      "Iteration 389, loss = 0.38643123\n",
      "Iteration 390, loss = 0.38518955\n",
      "Iteration 391, loss = 0.38053328\n",
      "Iteration 392, loss = 0.38052479\n",
      "Iteration 393, loss = 0.38386548\n",
      "Iteration 394, loss = 0.37585170\n",
      "Iteration 395, loss = 0.37370788\n",
      "Iteration 396, loss = 0.37467393\n",
      "Iteration 397, loss = 0.37536124\n",
      "Iteration 398, loss = 0.36796897\n",
      "Iteration 399, loss = 0.36643479\n",
      "Iteration 400, loss = 0.36476257\n",
      "Iteration 401, loss = 0.36223191\n",
      "Iteration 402, loss = 0.36095353\n",
      "Iteration 403, loss = 0.35915519\n",
      "Iteration 404, loss = 0.35983940\n",
      "Iteration 405, loss = 0.35541206\n",
      "Iteration 406, loss = 0.35376939\n",
      "Iteration 407, loss = 0.35219277\n",
      "Iteration 408, loss = 0.34972124\n",
      "Iteration 409, loss = 0.34928541\n",
      "Iteration 410, loss = 0.34622573\n",
      "Iteration 411, loss = 0.34599115\n",
      "Iteration 412, loss = 0.34360036\n",
      "Iteration 413, loss = 0.34283743\n",
      "Iteration 414, loss = 0.33909020\n",
      "Iteration 415, loss = 0.33893911\n",
      "Iteration 416, loss = 0.33570640\n",
      "Iteration 417, loss = 0.33448935\n",
      "Iteration 418, loss = 0.33307060\n",
      "Iteration 419, loss = 0.33047363\n",
      "Iteration 420, loss = 0.32946957\n",
      "Iteration 421, loss = 0.32795434\n",
      "Iteration 422, loss = 0.32653391\n",
      "Iteration 423, loss = 0.32521014\n",
      "Iteration 424, loss = 0.32355811\n",
      "Iteration 425, loss = 0.32116399\n",
      "Iteration 426, loss = 0.32216402\n",
      "Iteration 427, loss = 0.31893240\n",
      "Iteration 428, loss = 0.31564350\n",
      "Iteration 429, loss = 0.31406490\n",
      "Iteration 430, loss = 0.31399976\n",
      "Iteration 431, loss = 0.31136404\n",
      "Iteration 432, loss = 0.31135380\n",
      "Iteration 433, loss = 0.30905863\n",
      "Iteration 434, loss = 0.30742437\n",
      "Iteration 435, loss = 0.30498461\n",
      "Iteration 436, loss = 0.30356697\n",
      "Iteration 437, loss = 0.30198940\n",
      "Iteration 438, loss = 0.30010434\n",
      "Iteration 439, loss = 0.30103715\n",
      "Iteration 440, loss = 0.29704823\n",
      "Iteration 441, loss = 0.29653483\n",
      "Iteration 442, loss = 0.29352233\n",
      "Iteration 443, loss = 0.29439523\n",
      "Iteration 444, loss = 0.29116170\n",
      "Iteration 445, loss = 0.29133604\n",
      "Iteration 446, loss = 0.28779642\n",
      "Iteration 447, loss = 0.28703090\n",
      "Iteration 448, loss = 0.28624016\n",
      "Iteration 449, loss = 0.28357887\n",
      "Iteration 450, loss = 0.28253912\n",
      "Iteration 451, loss = 0.28092956\n",
      "Iteration 452, loss = 0.28044968\n",
      "Iteration 453, loss = 0.27808508\n",
      "Iteration 454, loss = 0.27912816\n",
      "Iteration 455, loss = 0.27604782\n",
      "Iteration 456, loss = 0.28392907\n",
      "Iteration 457, loss = 0.27452501\n",
      "Iteration 458, loss = 0.27147031\n",
      "Iteration 459, loss = 0.27512127\n",
      "Iteration 460, loss = 0.26815727\n",
      "Iteration 461, loss = 0.26711015\n",
      "Iteration 462, loss = 0.26540417\n",
      "Iteration 463, loss = 0.26439414\n",
      "Iteration 464, loss = 0.26283725\n",
      "Iteration 465, loss = 0.26306354\n",
      "Iteration 466, loss = 0.26022817\n",
      "Iteration 467, loss = 0.25927113\n",
      "Iteration 468, loss = 0.25781342\n",
      "Iteration 469, loss = 0.25848922\n",
      "Iteration 470, loss = 0.25777882\n",
      "Iteration 471, loss = 0.25531263\n",
      "Iteration 472, loss = 0.25294924\n",
      "Iteration 473, loss = 0.25350787\n",
      "Iteration 474, loss = 0.25143560\n",
      "Iteration 475, loss = 0.25029565\n",
      "Iteration 476, loss = 0.25134880\n",
      "Iteration 477, loss = 0.24990823\n",
      "Iteration 478, loss = 0.24633309\n",
      "Iteration 479, loss = 0.24889113\n",
      "Iteration 480, loss = 0.24546756\n",
      "Iteration 481, loss = 0.24278220\n",
      "Iteration 482, loss = 0.24164566\n",
      "Iteration 483, loss = 0.24614043\n",
      "Iteration 484, loss = 0.23905457\n",
      "Iteration 485, loss = 0.23863740\n",
      "Iteration 486, loss = 0.24006782\n",
      "Iteration 487, loss = 0.23526341\n",
      "Iteration 488, loss = 0.23574592\n",
      "Iteration 489, loss = 0.23408613\n",
      "Iteration 490, loss = 0.23312031\n",
      "Iteration 491, loss = 0.23110674\n",
      "Iteration 492, loss = 0.23032254\n",
      "Iteration 493, loss = 0.22948364\n",
      "Iteration 494, loss = 0.22824562\n",
      "Iteration 495, loss = 0.22758761\n",
      "Iteration 496, loss = 0.22610052\n",
      "Iteration 497, loss = 0.22614335\n",
      "Iteration 498, loss = 0.22467800\n",
      "Iteration 499, loss = 0.22390599\n",
      "Iteration 500, loss = 0.22237642\n",
      "Iteration 501, loss = 0.22257915\n",
      "Iteration 502, loss = 0.22024911\n",
      "Iteration 503, loss = 0.22425099\n",
      "Iteration 504, loss = 0.21937564\n",
      "Iteration 505, loss = 0.21684696\n",
      "Iteration 506, loss = 0.21745241\n",
      "Iteration 507, loss = 0.21604523\n",
      "Iteration 508, loss = 0.21862187\n",
      "Iteration 509, loss = 0.21403292\n",
      "Iteration 510, loss = 0.21368457\n",
      "Iteration 511, loss = 0.21353660\n",
      "Iteration 512, loss = 0.21319515\n",
      "Iteration 513, loss = 0.21041358\n",
      "Iteration 514, loss = 0.20990056\n",
      "Iteration 515, loss = 0.21331055\n",
      "Iteration 516, loss = 0.20876571\n",
      "Iteration 517, loss = 0.21801900\n",
      "Iteration 518, loss = 0.20862052\n",
      "Iteration 519, loss = 0.21669234\n",
      "Iteration 520, loss = 0.21216326\n",
      "Iteration 521, loss = 0.20298185\n",
      "Iteration 522, loss = 0.20606357\n",
      "Iteration 523, loss = 0.20208776\n",
      "Iteration 524, loss = 0.20057786\n",
      "Iteration 525, loss = 0.20167384\n",
      "Iteration 526, loss = 0.19967943\n",
      "Iteration 527, loss = 0.20335546\n",
      "Iteration 528, loss = 0.19976549\n",
      "Iteration 529, loss = 0.19833875\n",
      "Iteration 530, loss = 0.19836089\n",
      "Iteration 531, loss = 0.19593821\n",
      "Iteration 532, loss = 0.19778154\n",
      "Iteration 533, loss = 0.19816242\n",
      "Iteration 534, loss = 0.19411626\n",
      "Iteration 535, loss = 0.19324799\n",
      "Iteration 536, loss = 0.19392937\n",
      "Iteration 537, loss = 0.19268860\n",
      "Iteration 538, loss = 0.18988909\n",
      "Iteration 539, loss = 0.19105476\n",
      "Iteration 540, loss = 0.19119410\n",
      "Iteration 541, loss = 0.19205591\n",
      "Iteration 542, loss = 0.19223099\n",
      "Iteration 543, loss = 0.18718590\n",
      "Iteration 544, loss = 0.19005274\n",
      "Iteration 545, loss = 0.18771876\n",
      "Iteration 546, loss = 0.18758121\n",
      "Iteration 547, loss = 0.18596639\n",
      "Iteration 548, loss = 0.18563022\n",
      "Iteration 549, loss = 0.18400323\n",
      "Iteration 550, loss = 0.18351295\n",
      "Iteration 551, loss = 0.18187160\n",
      "Iteration 552, loss = 0.18192703\n",
      "Iteration 553, loss = 0.18124043\n",
      "Iteration 554, loss = 0.18102402\n",
      "Iteration 555, loss = 0.18287560\n",
      "Iteration 556, loss = 0.18088456\n",
      "Iteration 557, loss = 0.18121299\n",
      "Iteration 558, loss = 0.18075458\n",
      "Iteration 559, loss = 0.17937944\n",
      "Iteration 560, loss = 0.17860802\n",
      "Iteration 561, loss = 0.17731323\n",
      "Iteration 562, loss = 0.17616646\n",
      "Iteration 563, loss = 0.17538784\n",
      "Iteration 564, loss = 0.17498986\n",
      "Iteration 565, loss = 0.17532849\n",
      "Iteration 566, loss = 0.17359495\n",
      "Iteration 567, loss = 0.17479656\n",
      "Iteration 568, loss = 0.17282697\n",
      "Iteration 569, loss = 0.17255530\n",
      "Iteration 570, loss = 0.17178987\n",
      "Iteration 571, loss = 0.17317142\n",
      "Iteration 572, loss = 0.17114167\n",
      "Iteration 573, loss = 0.17054827\n",
      "Iteration 574, loss = 0.17055980\n",
      "Iteration 575, loss = 0.16941174\n",
      "Iteration 576, loss = 0.17287132\n",
      "Iteration 577, loss = 0.16983942\n",
      "Iteration 578, loss = 0.16883904\n",
      "Iteration 579, loss = 0.16834176\n",
      "Iteration 580, loss = 0.16759098\n",
      "Iteration 581, loss = 0.16835727\n",
      "Iteration 582, loss = 0.17370793\n",
      "Iteration 583, loss = 0.16762518\n",
      "Iteration 584, loss = 0.16847524\n",
      "Iteration 585, loss = 0.16724159\n",
      "Iteration 586, loss = 0.16649963\n",
      "Iteration 587, loss = 0.16738371\n",
      "Iteration 588, loss = 0.16489165\n",
      "Iteration 589, loss = 0.16447393\n",
      "Iteration 590, loss = 0.16944420\n",
      "Iteration 591, loss = 0.16556357\n",
      "Iteration 592, loss = 0.16549001\n",
      "Iteration 593, loss = 0.16317392\n",
      "Iteration 594, loss = 0.16381765\n",
      "Iteration 595, loss = 0.16261328\n",
      "Iteration 596, loss = 0.16268969\n",
      "Iteration 597, loss = 0.16144695\n",
      "Iteration 598, loss = 0.16412684\n",
      "Iteration 599, loss = 0.16339967\n",
      "Iteration 600, loss = 0.16062966\n",
      "Iteration 601, loss = 0.16016835\n",
      "Iteration 602, loss = 0.16142284\n",
      "Iteration 603, loss = 0.15915238\n",
      "Iteration 604, loss = 0.16174647\n",
      "Iteration 605, loss = 0.16006210\n",
      "Iteration 606, loss = 0.16080354\n",
      "Iteration 607, loss = 0.16128188\n",
      "Iteration 608, loss = 0.16013346\n",
      "Iteration 609, loss = 0.16282701\n",
      "Iteration 610, loss = 0.15560282\n",
      "Iteration 611, loss = 0.16183587\n",
      "Iteration 612, loss = 0.15878661\n",
      "Iteration 613, loss = 0.15927630\n",
      "Iteration 614, loss = 0.16164877\n",
      "Iteration 615, loss = 0.15685470\n",
      "Iteration 616, loss = 0.15645230\n",
      "Iteration 617, loss = 0.15586898\n",
      "Iteration 618, loss = 0.15649461\n",
      "Iteration 619, loss = 0.15478504\n",
      "Iteration 620, loss = 0.15545823\n",
      "Iteration 621, loss = 0.15424715\n",
      "Iteration 622, loss = 0.15852157\n",
      "Iteration 623, loss = 0.15979584\n",
      "Iteration 624, loss = 0.15506939\n",
      "Iteration 625, loss = 0.15322748\n",
      "Iteration 626, loss = 0.15404314\n",
      "Iteration 627, loss = 0.15392946\n",
      "Iteration 628, loss = 0.15449731\n",
      "Iteration 629, loss = 0.15482588\n",
      "Iteration 630, loss = 0.15369728\n",
      "Iteration 631, loss = 0.15244181\n",
      "Iteration 632, loss = 0.16112916\n",
      "Iteration 633, loss = 0.15295630\n",
      "Iteration 634, loss = 0.15221175\n",
      "Iteration 635, loss = 0.15382539\n",
      "Iteration 636, loss = 0.15265441\n",
      "Iteration 637, loss = 0.15310701\n",
      "Iteration 638, loss = 0.15453259\n",
      "Iteration 639, loss = 0.15310212\n",
      "Iteration 640, loss = 0.15171713\n",
      "Iteration 641, loss = 0.15127953\n",
      "Iteration 642, loss = 0.15104745\n",
      "Iteration 643, loss = 0.14960363\n",
      "Iteration 644, loss = 0.15010745\n",
      "Iteration 645, loss = 0.15026347\n",
      "Iteration 646, loss = 0.15047008\n",
      "Iteration 647, loss = 0.14978059\n",
      "Iteration 648, loss = 0.14851708\n",
      "Iteration 649, loss = 0.15044102\n",
      "Iteration 650, loss = 0.15407892\n",
      "Iteration 651, loss = 0.15040965\n",
      "Iteration 652, loss = 0.14825690\n",
      "Iteration 653, loss = 0.14879237\n",
      "Iteration 654, loss = 0.14727470\n",
      "Iteration 655, loss = 0.15596602\n",
      "Iteration 656, loss = 0.14740065\n",
      "Iteration 657, loss = 0.14773739\n",
      "Iteration 658, loss = 0.14666536\n",
      "Iteration 659, loss = 0.15006307\n",
      "Iteration 660, loss = 0.15204339\n",
      "Iteration 661, loss = 0.14889700\n",
      "Iteration 662, loss = 0.14648015\n",
      "Iteration 663, loss = 0.15063621\n",
      "Iteration 664, loss = 0.14663538\n",
      "Iteration 665, loss = 0.14673545\n",
      "Iteration 666, loss = 0.14759459\n",
      "Iteration 667, loss = 0.14650216\n",
      "Iteration 668, loss = 0.14667400\n",
      "Iteration 669, loss = 0.14472551\n",
      "Iteration 670, loss = 0.14546588\n",
      "Iteration 671, loss = 0.14634563\n",
      "Iteration 672, loss = 0.14565415\n",
      "Iteration 673, loss = 0.14764943\n",
      "Iteration 674, loss = 0.14610529\n",
      "Iteration 675, loss = 0.14403793\n",
      "Iteration 676, loss = 0.14648480\n",
      "Iteration 677, loss = 0.14576256\n",
      "Iteration 678, loss = 0.14488240\n",
      "Iteration 679, loss = 0.14394844\n",
      "Iteration 680, loss = 0.14439449\n",
      "Iteration 681, loss = 0.14773914\n",
      "Iteration 682, loss = 0.14624733\n",
      "Iteration 683, loss = 0.14509144\n",
      "Iteration 684, loss = 0.14249436\n",
      "Iteration 685, loss = 0.15328062\n",
      "Iteration 686, loss = 0.14229762\n",
      "Iteration 687, loss = 0.15042635\n",
      "Iteration 688, loss = 0.14663002\n",
      "Iteration 689, loss = 0.14643089\n",
      "Iteration 690, loss = 0.14328263\n",
      "Iteration 691, loss = 0.14438913\n",
      "Iteration 692, loss = 0.14424205\n",
      "Iteration 693, loss = 0.14299076\n",
      "Iteration 694, loss = 0.14836020\n",
      "Iteration 695, loss = 0.14189266\n",
      "Iteration 696, loss = 0.14181045\n",
      "Iteration 697, loss = 0.14089866\n",
      "Iteration 698, loss = 0.14719220\n",
      "Iteration 699, loss = 0.14359654\n",
      "Iteration 700, loss = 0.14375423\n",
      "Iteration 701, loss = 0.13993532\n",
      "Iteration 702, loss = 0.14229716\n",
      "Iteration 703, loss = 0.14134555\n",
      "Iteration 704, loss = 0.14214048\n",
      "Iteration 705, loss = 0.13951206\n",
      "Iteration 706, loss = 0.14152414\n",
      "Iteration 707, loss = 0.14137287\n",
      "Iteration 708, loss = 0.14247168\n",
      "Iteration 709, loss = 0.14245259\n",
      "Iteration 710, loss = 0.14258817\n",
      "Iteration 711, loss = 0.14505798\n",
      "Iteration 712, loss = 0.14044949\n",
      "Iteration 713, loss = 0.14228044\n",
      "Iteration 714, loss = 0.13872448\n",
      "Iteration 715, loss = 0.14049079\n",
      "Iteration 716, loss = 0.13894674\n",
      "Iteration 717, loss = 0.13875762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 718, loss = 0.14203440\n",
      "Iteration 719, loss = 0.14332113\n",
      "Iteration 720, loss = 0.14385090\n",
      "Iteration 721, loss = 0.14525261\n",
      "Iteration 722, loss = 0.13714222\n",
      "Iteration 723, loss = 0.14545401\n",
      "Iteration 724, loss = 0.13841098\n",
      "Iteration 725, loss = 0.14294026\n",
      "Iteration 726, loss = 0.14022644\n",
      "Iteration 727, loss = 0.14108198\n",
      "Iteration 728, loss = 0.13943297\n",
      "Iteration 729, loss = 0.14053347\n",
      "Iteration 730, loss = 0.13987547\n",
      "Iteration 731, loss = 0.13873639\n",
      "Iteration 732, loss = 0.13757948\n",
      "Iteration 733, loss = 0.13733801\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='identity', alpha=0.0001, batch_size='auto',\n",
       "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=10, learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=True,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = \"identity\"\n",
    "#activation = \"logistic\"\n",
    "#activation = \"tanh\"\n",
    "#activation = \"relu\"\n",
    "mlp = MLPClassifier(#random_state=42,\n",
    "                    hidden_layer_sizes=(10),\n",
    "                    max_iter = 1000,\n",
    "                    activation = activation,\n",
    "                    verbose = True\n",
    "                    )\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿Es posible clasificar correctamente los casos utilizando un Perceptron?\n",
    "- ¿Cuántas iteraciones son necesarias?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si es posible, son necesarias 212 iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de iteraciones: 733\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Cantidad de iteraciones: \" +str(mlp.n_iter_))\n",
    "disp = metrics.plot_confusion_matrix(mlp, X_test, y_test,cmap=plt.cm.Blues)\n",
    "disp.ax_.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
